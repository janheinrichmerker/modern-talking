\documentclass[english,handout]{mlutalk}

\title{Stance Classification for Key-Point Analysis}
% \title{%
%   Modern Talking: Key-Point Analysis \\
%   using Modern Natural Language Processing
% }
\subtitle{Natural Language Processing, Summer Semester 2021}
\author{Max Henze \and Hanh Luu \and Jan Heinrich Reimer}
\institute{Martin Luther University Halle-Wittenberg}
\date{\today}
\titlegraphic{\includegraphics[width=3cm]{figures/mlu-halle}}

\addbibresource{../literature/literature.bib}

\usepackage{tikz}
\usepackage{listings}
\usepackage{xspace}
\usepackage{biblatex}

\newcommand{\Bert}{\textsc{Bert}\xspace}
\newcommand{\ArgKP}{\mbox{ArgKP}\xspace}
\newcommand{\ArgQ}{\mbox{IBM-ArgQ-Rank-30kArgs}\xspace}
\newcommand{\BiLSTM}{\mbox{BiLSTM}\xspace}
\newcommand{\BertBase}{\textsc{Bert}-Base\xspace}
\newcommand{\BertLarge}{\textsc{Bert}-Large\xspace}
\newcommand{\TF}{\mbox{TF}\xspace}
\newcommand{\TFIDF}{\mbox{TF/IDF}\xspace}

\begin{document}

\titleframe

\begin{frame}{Stance Classification} 
  \begin{block}{Motivation}
    \begin{itemize}
      \item Given an argument A and a topic T: Can A be classified as pro, con or something else to T.
    \end{itemize}
  \end{block}
  \begin{block}{Problem}
    \begin{itemize}
      \item What makes an argument pro, con or else ? 
      \item Can an argument be classified pro by looking at other pro-like arguments ?
    \end{itemize}
  \end{block}
  \begin{example}
    \begin{tabular}{l|l}
      T & \multicolumn{1}{c}{Nuclear Energy}\\
      \hline
      $A_1$ & "Nuclear energy is a cheap alternative to fossil fuels."\\
      $A_2$ & "Failures in nuclear power plants resolve in catastrophic events."\\
      $A_3$ & "The CDU decided on a nuclear phase-out in 2021/22."\\
      $A_4$ & "I don't like nuclear energy because my mom also doesn't like it."
    \end{tabular}
  \end{example}
\end{frame}

\begin{frame}{Approaches} % Hanh
  % Mathematical framework
  TODO: general framework and basic approaches.
\end{frame}

\begin{frame}{Fake News Challenge Stance Detection~\cite{HanselowskiSSCC2018}}
  \begin{example}
    % Sample document for different stances
    % Sample headline
  \end{example}
  Challenge: % Try to use blocks \begin{block}{Title} ... \end{block}
  \begin{itemize}
      \item Stance $s$: argee, disgree, discuss, unrelated
      \item Predict a label stance for a document with regard to a headline: $f: (d,h) \rightarrow s$
  \end{itemize}
  Topic of this paper:
  \begin{itemize}
    \item Reproduction and Analysis for the top-performing systems of the challenge: \textit{Tablos, Athene, UCL}
    \item Question: Which features and architectures used help improving performance?
    \item (?) Imbalanced Learning, in-domain and cross-domain (transfer)
  \end{itemize}
\end{frame}
% Try to continue slide with frame breaks

\begin{frame}{Fake News Challenge Stance Detection~\cite{HanselowskiSSCC2018}}
    \begin{table}[]
    \caption{The best approaches}
        \label{tab:comparison}
        \begin{tabular}{|l|l|l|l|}
        \hline
        \multicolumn{2}{|l|}{Talos} &
          Athene &
          UCL \\ \hline
        \multicolumn{2}{|l|}{\begin{tabular}[c]{@{}l@{}}weighted model\\ (CNN un GBD Tree)\end{tabular}} &
          \begin{tabular}[c]{@{}l@{}}MLP \\ (6 h-layers+ Softmax)\end{tabular} &
          MLP (1 h-layer) \\ \hline
        word2vec &
          \begin{tabular}[c]{@{}l@{}}TF\\ TFIDF\\ Sentiment \\ word2vec\end{tabular} &
          \begin{tabular}[c]{@{}l@{}}Unigrams\\ Similarity (Noun, Verbs)\\ Topic Models\end{tabular} &
          \begin{tabular}[c]{@{}l@{}}TF of unigrams\\ TFIDF \\ Similarity\end{tabular} \\ \hline
        \end{tabular}
        % Maybe swap columns / rows?
    \end{table}
\end{frame}
% Try to continue slide with frame breaks

\begin{frame}{Fake News Challenge Stance Detection~\cite{HanselowskiSSCC2018}}
   Which features and models help improving performance?
   ...
\end{frame}

\begin{frame}[allowframebreaks]{\Bert Same Side Stance Classification~\cite{OllingerDSBS2020}}
  
  \begin{block}{Problem}
    Same side  Stance Classification
    \begin{itemize}
      \item Are both arguments pro or both con?
      \item Simplification of general stance classification
      \item 
    \end{itemize}
  \end{block}

  \begin{example}
    % Add topic for sample
    % Example structure: topic and sample arguments (claim, premise)
    \begin{align}
      p_1 &= \text{"Renewable energy can soon replace fossil/nuclear energy"} \\
      p_2 &= \text{"Nuclear energy is a cheap alternative to fossil fuels"} \\
      p_3 &= \text{"The danger from radioactive contamination should be avoided"}
    \end{align}
    Sentences \(p_1\) and \(p_2\) have different stances, \(p_1\) and \(p_3\) have the same.
  \end{example}
  
  \framebreak
  
  \begin{block}{Approach}
    \begin{itemize}
      \item \Bert architecture for classification
      \item Fine-tune \BertBase and \BertLarge models for 3~epochs
      \item Sequences limited to 512 tokens (due to position embedding length limit)
    \end{itemize}
  \end{block}

  \begin{block}{Results}
    \begin{itemize}
      \item \Bert classifier outperforms SVM baseline
      \item \BertLarge slightly better than \BertBase
      \item Longer sequences (i.e., less truncated) do not perform much better \\ (most sequences are short anyways)
      \item Classifier with \Bert can learn from partially truncated sentences
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}{Cross-topic Argument Mining~\cite{StabMSRG2018}}
  Topic of this paper:
  \begin{itemize}
    \item To search for arguments, relevant to a certain topic 
    \item Given a heterogeneous document collection
  \end{itemize}
  Key Achievements found in this paper:
  \begin{itemize}
    \item Non, supporting and opposing styled annotation scheme
    \item New corpus including 27,520 sentences for 8 controversial topics
    \item Contextual BiLSTM leads to better generalization to unknown topics
    \item Multi-task learning setup further improved this approach 
    % Topic information during training improves prediction?
    % Problems?
  \end{itemize}
\end{frame}
% Try to continue slide with frame breaks

\begin{frame}{Cross-topic Argument Mining~\cite{StabMSRG2018}}
  How did they achieve this ?\\ % Try to use blocks \begin{block}{Title} ... \end{block}
  $\rightarrow$ two major approaches for identifying arguments
  \begin{itemize}
    \item Integrating topic information
    \begin{itemize}
      \item Outer-attention BiLSTM 
      \begin{itemize}
        \item Model learning how to weight input words given the topic combined with BiLSTM
      \end{itemize}
      \item Custom contextual BiLSTM
      \begin{itemize}
        \item Adding topic information as additional input term 
      \end{itemize}
    \end{itemize}
    \item Leveraging additional data (two additional corpora)
    \begin{itemize}
      \item Transfer learning
      \begin{itemize}
        \item train model twice with auxiliary corpus first 
      \end{itemize}
      \item Multi-task learning
      \begin{itemize}
        \item using RNN for separate learning and combination afterwards
      \end{itemize}
    \end{itemize}
  \end{itemize}
  % Maybe "flatten" the key points a bit
\end{frame}

\begin{frame}{Comparison} % Hanh
  TODO: compare approaches
\end{frame}

\begin{frame}{Stance Classification in Key Point Matching} % Heini
  TODO: how can we use stance classification for matching key points?
  \thankyou
\end{frame}

% \begin{frame}{Conclusion}
%   TODO: what to do next?
% \end{frame}

% \begin{frame}{Future Work}
%   TODO: do we need future work?
%   \thankyou
% \end{frame}

\appendix
\section{\appendixname}

\bibliographyframe

\end{document}
